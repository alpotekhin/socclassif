{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray.tune import JupyterNotebookReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether cuda is on\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining small ruBert tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8049d2a1ed22d6e9\n",
      "Found cached dataset csv (C:/Users/al_po/.cache/huggingface/datasets/csv/default-8049d2a1ed22d6e9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d641b26fdc4d2bb963c0ff1dfefb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading dataset and fixing false 'train_test_split'\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"data/text-target.csv\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.rename_column(\"target\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining preprocessing for text to get embedding lookup table and attention\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/al_po/.cache/huggingface/datasets/csv/default-8049d2a1ed22d6e9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-b27e36eba33aa001.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving only the necessary for training columns\n",
    "\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=16)\n",
    "test_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will max the f1-macro \n",
    "\n",
    "metric = evaluate.load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for trainer evaluation\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hp search we need to use model_init=... instead of model=...\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"cointegrated/rubert-tiny2\", num_labels=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring Trainer\n",
    "# Check 'rubert_hp_search' for hyperparameter search example\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"rubert_hp\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=True,\n",
    "    log_level=\"error\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    model_init=model_init,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for ray.tune\n",
    "\n",
    "tune_config = {\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"num_train_epochs\": tune.choice([2, 3, 4, 5]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"eval_f1\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=1,\n",
    "    hyperparam_mutations={\n",
    "        \"weight_decay\": tune.uniform(0.0, 0.3),\n",
    "        \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-04-16 02:19:09</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:00.05        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.2/31.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Resources requested: 1.0/12 CPUs, 1.0/1 GPUs, 0.0/12.97 GiB heap, 0.0/6.49 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  w_decay</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_epochs</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_objective_ec7e7_00000</td><td>RUNNING </td><td>127.0.0.1:19812</td><td style=\"text-align: right;\"> 0.238963</td><td style=\"text-align: right;\">1.73374e-05</td><td style=\"text-align: right;\">           4</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reporter = JupyterNotebookReporter(\n",
    "    parameter_columns={\n",
    "        \"weight_decay\": \"w_decay\",\n",
    "        \"learning_rate\": \"lr\",\n",
    "        \"num_train_epochs\": \"num_epochs\",\n",
    "    },\n",
    "    metric_columns=[\"eval_f1\", \"eval_loss\", \"epoch\", \"training_iteration\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 02:19:06,303\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "c:\\Users\\al_po\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ray\\tune\\tune.py:562: UserWarning: Consider boosting PBT performance by enabling `reuse_actors` as well as implementing `reset_config` for Trainable.\n",
      "  warnings.warn(\n",
      "c:\\Users\\al_po\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py:612: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:\n",
      "\n",
      "from ray.air import session\n",
      "\n",
      "def train(config):\n",
      "    # ...\n",
      "    session.report({\"metric\": metric}, checkpoint=checkpoint)\n",
      "\n",
      "For more information please see https://docs.ray.io/en/master/tune/api_docs/trainable.html\n",
      "\n",
      "  warnings.warn(\n",
      "2023-04-16 02:19:09,030\tWARNING trial_runner.py:1677 -- You are trying to access _search_alg interface of TrialRunner in TrialScheduler, which is being restricted. If you believe it is reasonable for your scheduler to access this TrialRunner API, please reach out to Ray team on GitHub. A more strict API access pattern would be enforced starting 1.12s.0\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m c:\\Users\\al_po\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.7901, 'learning_rate': 1.6448476931262298e-05, 'epoch': 0.21}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.5275, 'learning_rate': 1.555956226787804e-05, 'epoch': 0.41}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.528, 'learning_rate': 1.4670647604493784e-05, 'epoch': 0.62}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.5432, 'learning_rate': 1.3781732941109525e-05, 'epoch': 0.82}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'eval_loss': 0.5704401135444641, 'eval_f1': 0.8468366765174079, 'eval_runtime': 76.8736, 'eval_samples_per_second': 42.277, 'eval_steps_per_second': 10.576, 'epoch': 1.0}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.5339, 'learning_rate': 1.2892818277725268e-05, 'epoch': 1.03}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.5206, 'learning_rate': 1.200390361434101e-05, 'epoch': 1.23}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.5523, 'learning_rate': 1.1114988950956753e-05, 'epoch': 1.44}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.4877, 'learning_rate': 1.0226074287572498e-05, 'epoch': 1.64}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.4663, 'learning_rate': 9.337159624188239e-06, 'epoch': 1.85}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'eval_loss': 0.5889736413955688, 'eval_f1': 0.8529945449078001, 'eval_runtime': 76.9228, 'eval_samples_per_second': 42.25, 'eval_steps_per_second': 10.569, 'epoch': 2.0}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.4603, 'learning_rate': 8.448244960803982e-06, 'epoch': 2.05}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.4469, 'learning_rate': 7.559330297419724e-06, 'epoch': 2.26}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.4173, 'learning_rate': 6.670415634035467e-06, 'epoch': 2.46}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.4783, 'learning_rate': 5.7815009706512095e-06, 'epoch': 2.67}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.5211, 'learning_rate': 4.8925863072669525e-06, 'epoch': 2.87}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'eval_loss': 0.5955279469490051, 'eval_f1': 0.851303986131622, 'eval_runtime': 81.0811, 'eval_samples_per_second': 40.083, 'eval_steps_per_second': 10.027, 'epoch': 3.0}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.4399, 'learning_rate': 4.003671643882695e-06, 'epoch': 3.08}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.4083, 'learning_rate': 3.114756980498438e-06, 'epoch': 3.28}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.4377, 'learning_rate': 2.2258423171141805e-06, 'epoch': 3.49}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.3878, 'learning_rate': 1.336927653729923e-06, 'epoch': 3.69}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'loss': 0.418, 'learning_rate': 4.480129903456657e-07, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 03:10:43,058\tINFO tune.py:798 -- Total run time: 3094.05 seconds (3094.02 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    hp_space=lambda _: tune_config,\n",
    "    backend=\"ray\",\n",
    "    n_trials=1,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    keep_checkpoints_num=1,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'eval_loss': 0.6095167398452759, 'eval_f1': 0.8554960240823112, 'eval_runtime': 79.67, 'eval_samples_per_second': 40.793, 'eval_steps_per_second': 10.205, 'epoch': 4.0}\n",
      "\u001b[2m\u001b[36m(_objective pid=19812)\u001b[0m {'train_runtime': 3089.9594, 'train_samples_per_second': 12.622, 'train_steps_per_second': 3.156, 'train_loss': 0.48905094106046754, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='ec7e7_00000', objective=0.8554960240823112, hyperparameters={'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4, 'num_train_epochs': 4, 'weight_decay': 0.23896289605806983, 'learning_rate': 1.7337391594646555e-05})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
